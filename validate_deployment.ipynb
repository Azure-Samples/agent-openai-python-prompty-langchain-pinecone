{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = \"<subscription-id>\"\n",
    "RESOURCE_GROUP = \"<resource-group>\"\n",
    "AML_WORKSPACE_NAME = \"<ai-project-name>\"\n",
    "ONLINE_ENDPOINT_NAME = \"<online-endpoint>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import (\n",
    "    DefaultAzureCredential,\n",
    "    InteractiveBrowserCredential,\n",
    "    AzureCliCredential,\n",
    ")\n",
    "from azure.ai.ml import (\n",
    "    MLClient,\n",
    ")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential(additionally_allowed_tenants=[\"*\"])\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential(additionally_allowed_tenants=[\"*\"])\n",
    "\n",
    "ml_client = MLClient(credential, SUBSCRIPTION_ID, RESOURCE_GROUP, AML_WORKSPACE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!az account set --subscription {SUBSCRIPTION_ID}\n",
    "!az configure --defaults workspace={AML_WORKSPACE_NAME} group={RESOURCE_GROUP}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    ")\n",
    "\n",
    "# get the online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=ONLINE_ENDPOINT_NAME\n",
    ")\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can customize your inference experience following Langserve instruction. The below code is just a simple example.\n",
    "from langserve import RemoteRunnable\n",
    "token = ml_client.online_endpoints.get_keys(name=endpoint.name).primary_key\n",
    "runnable_az = RemoteRunnable(f\"{endpoint.scoring_uri}openai-functions-agent\", headers={\"Authorization\": \"Bearer \" + token})\n",
    "async for msg in runnable_az.astream({\n",
    "    \"chat_history\": [],  \n",
    "    \"input\": \"what is the work from home policy from langchain-test-index?\"\n",
    "}):\n",
    "    print(msg, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
